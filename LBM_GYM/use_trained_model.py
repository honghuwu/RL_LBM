#!/usr/bin/env python3
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„PPOæ¨¡å‹è¿›è¡Œé¢„æµ‹å’Œè¯„ä¼°
å±•ç¤ºå¦‚ä½•åŠ è½½æ¨¡å‹ã€è¿›è¡Œæ¨ç†ã€è¯„ä¼°æ€§èƒ½å’Œå¯è§†åŒ–ç»“æœ
"""

import numpy as np
import matplotlib.pyplot as plt
import time
from stable_baselines3 import PPO
from gymnasium.wrappers import TimeLimit
from env_lbm import LBMEnv

class ModelEvaluator:
    """è®­ç»ƒå¥½çš„æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path, env_config=None):
        """
        åˆå§‹åŒ–æ¨¡å‹è¯„ä¼°å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            env_config: ç¯å¢ƒé…ç½®
        """
        self.model_path = model_path
        self.env_config = env_config or {"max_episode_steps": 200}
        
        # åŠ è½½æ¨¡å‹å’Œç¯å¢ƒ
        self.load_model_and_env()
        
        # å­˜å‚¨è¯„ä¼°ç»“æœ
        self.evaluation_results = {
            'rewards': [],
            'episode_lengths': [],
            'cd_values': [],
            'cl_values': [],
            'actions': [],
            'observations': []
        }
    
    def load_model_and_env(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œç¯å¢ƒ"""
        try:
            print("ğŸ”„ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
            
            # åˆ›å»ºç¯å¢ƒ
            base_env = LBMEnv(config=self.env_config)
            self.env = TimeLimit(base_env, max_episode_steps=self.env_config["max_episode_steps"])
            
            # åŠ è½½æ¨¡å‹
            self.model = PPO.load(self.model_path, env=self.env)
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            print(f"ğŸ“‹ ç¯å¢ƒä¿¡æ¯:")
            print(f"   - åŠ¨ä½œç©ºé—´: {self.env.action_space}")
            print(f"   - è§‚æµ‹ç©ºé—´: {self.env.observation_space.shape}")
            print(f"   - æœ€å¤§episodeé•¿åº¦: {self.env_config['max_episode_steps']}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def predict_single_action(self, observation, deterministic=True):
        """
        å¯¹å•ä¸ªè§‚æµ‹è¿›è¡Œé¢„æµ‹
        
        Args:
            observation: ç¯å¢ƒè§‚æµ‹
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            
        Returns:
            action: é¢„æµ‹çš„åŠ¨ä½œ
            action_prob: åŠ¨ä½œæ¦‚ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        """
        action, _states = self.model.predict(observation, deterministic=deterministic)
        return action
    
    def run_single_episode(self, render=False, deterministic=True, verbose=True):
        """
        è¿è¡Œå•ä¸ªepisode
        
        Args:
            render: æ˜¯å¦æ¸²æŸ“
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            episode_data: episodeçš„è¯¦ç»†æ•°æ®
        """
        obs, info = self.env.reset()
        
        episode_data = {
            'observations': [obs.copy()],
            'actions': [],
            'rewards': [],
            'infos': [info.copy()],
            'total_reward': 0,
            'episode_length': 0
        }
        
        if verbose:
            print(f"\nğŸ¯ å¼€å§‹æ–°çš„episode...")
        
        step_count = 0
        while True:
            # é¢„æµ‹åŠ¨ä½œ
            action = self.predict_single_action(obs, deterministic=deterministic)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = self.env.step(action)
            
            # è®°å½•æ•°æ®
            episode_data['actions'].append(action.copy())
            episode_data['rewards'].append(reward)
            episode_data['observations'].append(obs.copy())
            episode_data['infos'].append(info.copy())
            episode_data['total_reward'] += reward
            episode_data['episode_length'] += 1
            
            step_count += 1
            
            # æ‰“å°è¿›åº¦
            if verbose and step_count % 20 == 0:
                cd = info.get('CD', 0)
                cl = info.get('CL', 0)
                print(f"   ğŸ“Š æ­¥æ•°: {step_count} | åŠ¨ä½œ: {action[0]:.3f} | "
                      f"å¥–åŠ±: {reward:.4f} | CD: {cd:.4f} | CL: {cl:.4f}")
            
            # æ¸²æŸ“
            if render:
                self.env.render()
                time.sleep(0.05)
            
            # æ£€æŸ¥episodeç»“æŸ
            if terminated or truncated:
                end_reason = "è‡ªç„¶ç»“æŸ" if terminated else "è¾¾åˆ°æœ€å¤§æ­¥æ•°"
                if verbose:
                    print(f"   âœ… Episodeç»“æŸ: {end_reason}")
                    print(f"   ğŸ“ˆ æ€»æ­¥æ•°: {episode_data['episode_length']}")
                    print(f"   ğŸ† æ€»å¥–åŠ±: {episode_data['total_reward']:.4f}")
                break
        
        return episode_data
    
    def evaluate_model(self, num_episodes=10, deterministic=True, verbose=True):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            num_episodes: è¯„ä¼°çš„episodeæ•°é‡
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        print(f"\nğŸ§ª å¼€å§‹æ¨¡å‹è¯„ä¼° ({num_episodes} episodes)...")
        print("=" * 60)
        
        all_rewards = []
        all_lengths = []
        all_cd_values = []
        all_cl_values = []
        
        for episode in range(num_episodes):
            if verbose:
                print(f"\nğŸ“Š Episode {episode + 1}/{num_episodes}")
            
            # è¿è¡Œepisode
            episode_data = self.run_single_episode(
                render=False, 
                deterministic=deterministic, 
                verbose=verbose
            )
            
            # æ”¶é›†ç»Ÿè®¡æ•°æ®
            all_rewards.append(episode_data['total_reward'])
            all_lengths.append(episode_data['episode_length'])
            
            # æ”¶é›†ç‰©ç†å‚æ•°
            cd_values = [info.get('CD', 0) for info in episode_data['infos']]
            cl_values = [info.get('CL', 0) for info in episode_data['infos']]
            
            if cd_values:
                all_cd_values.extend(cd_values)
                all_cl_values.extend(cl_values)
        
        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        results = {
            'mean_reward': np.mean(all_rewards),
            'std_reward': np.std(all_rewards),
            'mean_length': np.mean(all_lengths),
            'std_length': np.std(all_lengths),
            'mean_cd': np.mean(all_cd_values) if all_cd_values else 0,
            'mean_cl': np.mean(all_cl_values) if all_cl_values else 0,
            'all_rewards': all_rewards,
            'all_lengths': all_lengths,
            'all_cd_values': all_cd_values,
            'all_cl_values': all_cl_values
        }
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ€»ç»“:")
        print(f"   ğŸ† å¹³å‡å¥–åŠ±: {results['mean_reward']:.4f} Â± {results['std_reward']:.4f}")
        print(f"   ğŸ“ å¹³å‡é•¿åº¦: {results['mean_length']:.1f} Â± {results['std_length']:.1f}")
        print(f"   ğŸŒªï¸  å¹³å‡é˜»åŠ›ç³»æ•°(CD): {results['mean_cd']:.4f}")
        print(f"   â¬†ï¸  å¹³å‡å‡åŠ›ç³»æ•°(CL): {results['mean_cl']:.4f}")
        if results['mean_cd'] > 0:
            print(f"   âš¡ å¹³å‡å‡é˜»æ¯”(CL/CD): {results['mean_cl']/results['mean_cd']:.4f}")
        print("=" * 60)
        
        return results
    
    def visualize_episode(self, render=True, save_data=True):
        """
        å¯è§†åŒ–å•ä¸ªepisodeçš„è¿è¡Œè¿‡ç¨‹
        
        Args:
            render: æ˜¯å¦å®æ—¶æ¸²æŸ“
            save_data: æ˜¯å¦ä¿å­˜æ•°æ®ç”¨äºåç»­åˆ†æ
        """
        print("\nğŸ¬ å¼€å§‹å¯è§†åŒ–episode...")
        
        episode_data = self.run_single_episode(render=render, verbose=True)
        
        if save_data:
            # ç»˜åˆ¶episodeæ•°æ®
            self.plot_episode_data(episode_data)
        
        return episode_data
    
    def plot_episode_data(self, episode_data):
        """ç»˜åˆ¶episodeæ•°æ®å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('Episode è¿è¡Œæ•°æ®åˆ†æ', fontsize=16)
        
        steps = range(len(episode_data['rewards']))
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(steps, episode_data['rewards'])
        axes[0, 0].set_title('Step Rewards')
        axes[0, 0].set_xlabel('Steps')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].grid(True)
        
        # åŠ¨ä½œæ›²çº¿
        actions = [action[0] for action in episode_data['actions']]
        axes[0, 1].plot(steps[:-1], actions)  # actionsæ¯”observationså°‘ä¸€ä¸ª
        axes[0, 1].set_title('Actions')
        axes[0, 1].set_xlabel('Steps')
        axes[0, 1].set_ylabel('Action Value')
        axes[0, 1].grid(True)
        
        # CDæ›²çº¿
        cd_values = [info.get('CD', 0) for info in episode_data['infos']]
        axes[1, 0].plot(steps, cd_values)
        axes[1, 0].set_title('Drag Coefficient (CD)')
        axes[1, 0].set_xlabel('Steps')
        axes[1, 0].set_ylabel('CD')
        axes[1, 0].grid(True)
        
        # CLæ›²çº¿
        cl_values = [info.get('CL', 0) for info in episode_data['infos']]
        axes[1, 1].plot(steps, cl_values)
        axes[1, 1].set_title('Lift Coefficient (CL)')
        axes[1, 1].set_xlabel('Steps')
        axes[1, 1].set_ylabel('CL')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig('./models/episode_analysis.png', dpi=300, bbox_inches='tight')
        print("ğŸ“ˆ Episodeåˆ†æå›¾å·²ä¿å­˜è‡³ ./models/episode_analysis.png")
        plt.show()
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if hasattr(self, 'env'):
            self.env.close()

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    # æ¨¡å‹è·¯å¾„ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    model_paths = [
        "./models/final_model.zip",      # æœ€ç»ˆæ¨¡å‹
        "./models/best_model/best_model.zip"  # æœ€ä½³æ¨¡å‹
    ]
    
    # é€‰æ‹©å¯ç”¨çš„æ¨¡å‹
    model_path = None
    for path in model_paths:
        try:
            import os
            if os.path.exists(path):
                model_path = path
                break
        except:
            continue
    
    if model_path is None:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ï¼")
        print("ğŸ’¡ è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€å­˜åœ¨ï¼š")
        for path in model_paths:
            print(f"   - {path}")
        print("\nğŸ”§ å¦‚æœè¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œ: python train_ppo.py")
        return
    
    try:
        # åˆ›å»ºæ¨¡å‹è¯„ä¼°å™¨
        evaluator = ModelEvaluator(model_path)
        
        print("\n" + "=" * 60)
        print("ğŸ¯ æ¨¡å‹ä½¿ç”¨é€‰é¡¹:")
        print("1. å¿«é€Ÿè¯„ä¼° (5 episodes)")
        print("2. è¯¦ç»†è¯„ä¼° (10 episodes)")
        print("3. å¯è§†åŒ–è¿è¡Œ (1 episode with rendering)")
        print("4. å•æ­¥é¢„æµ‹æ¼”ç¤º")
        print("=" * 60)
        
        choice = input("è¯·é€‰æ‹©æ“ä½œ (1-4): ").strip()
        
        if choice == "1":
            # å¿«é€Ÿè¯„ä¼°
            results = evaluator.evaluate_model(num_episodes=5, verbose=False)
            
        elif choice == "2":
            # è¯¦ç»†è¯„ä¼°
            results = evaluator.evaluate_model(num_episodes=10, verbose=True)
            
        elif choice == "3":
            # å¯è§†åŒ–è¿è¡Œ
            episode_data = evaluator.visualize_episode(render=True, save_data=True)
            
        elif choice == "4":
            # å•æ­¥é¢„æµ‹æ¼”ç¤º
            print("\nğŸ” å•æ­¥é¢„æµ‹æ¼”ç¤º...")
            obs, info = evaluator.env.reset()
            print(f"ğŸ“Š åˆå§‹è§‚æµ‹å½¢çŠ¶: {obs.shape}")
            
            for i in range(5):
                action = evaluator.predict_single_action(obs)
                print(f"   æ­¥éª¤ {i+1}: é¢„æµ‹åŠ¨ä½œ = {action[0]:.4f}")
                obs, reward, terminated, truncated, info = evaluator.env.step(action)
                print(f"           å¥–åŠ± = {reward:.4f}, CD = {info.get('CD', 0):.4f}, CL = {info.get('CL', 0):.4f}")
                if terminated or truncated:
                    break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼")
        
        # å…³é—­ç¯å¢ƒ
        evaluator.close()
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()