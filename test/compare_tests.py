#!/usr/bin/env python3
"""
å¯¹æ¯”æµ‹è¯•ï¼šä½¿ç”¨æ¨¡å‹ vs ä¸ä½¿ç”¨æ¨¡å‹
åŒæ—¶è¿è¡Œä¸¤ä¸ªæµ‹è¯•å¹¶å¯¹æ¯”ç»“æœ
"""

import numpy as np
import matplotlib.pyplot as plt
import os
import sys

from stable_baselines3 import PPO
from gymnasium.wrappers import TimeLimit

# æ·»åŠ PPOç›®å½•åˆ°Pythonè·¯å¾„ä»¥å¯¼å…¥env_lbm
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'PPO')))
from env_lbm import LBMEnv

def run_comparison_test(total_steps=5000):
    """
    è¿è¡Œå¯¹æ¯”æµ‹è¯•
    
    Args:
        total_steps: æ¯ä¸ªæµ‹è¯•çš„æ€»æ­¥æ•°
    """

    model_paths = [
        "../../models/final_model.zip",
        "../models/final_model.zip",
        "./models/final_model.zip",
    ]
    
    model_path = None

    for path in model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if model_path is None:
        print("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ï¼")
        return
    
    # æµ‹è¯•1: ä½¿ç”¨æ¨¡å‹
    print("\næµ‹è¯•1: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("-" * 40)
    
    base_env1 = LBMEnv(config={"max_episode_steps": 200})
    env1 = TimeLimit(base_env1, max_episode_steps=200)
    model = PPO.load(model_path, env=env1)
    
    model_rewards = []
    obs1, info1 = env1.reset()
    current_step1 = 0
    
    while current_step1 < total_steps:
        action, _states = model.predict(obs1, deterministic=True)
        obs1, reward, terminated, truncated, info1 = env1.step(action)
        model_rewards.append(reward)
        current_step1 += 1
        
        # å®æ—¶è¿›åº¦æ¡æ˜¾ç¤º
        if current_step1 % 100 == 0 or current_step1 == total_steps:
            progress = current_step1 / total_steps
            bar_length = 50
            filled_length = int(bar_length * progress)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            recent_avg = np.mean(model_rewards[-1000:]) if len(model_rewards) >= 1000 else np.mean(model_rewards)
            print(f"\ræ¨¡å‹æµ‹è¯• [{bar}] {current_step1}/{total_steps} ({progress:.1%}) | å¹³å‡å¥–åŠ±: {recent_avg:.4f}", end='', flush=True)
        
        if terminated or truncated:
            obs1, info1 = env1.reset()
    
    print()  # æ¢è¡Œï¼Œç¡®ä¿è¿›åº¦æ¡æ˜¾ç¤ºå®Œæ•´
    env1.close()
    
    # æµ‹è¯•2: æ— åŠ¨ä½œ
    print("\nâ¸ æµ‹è¯•2: æ— åŠ¨ä½œ")
    print("-" * 40)
    
    base_env2 = LBMEnv(config={"max_episode_steps": 200})
    env2 = TimeLimit(base_env2, max_episode_steps=200)
    
    no_action_rewards = []
    obs2, info2 = env2.reset()
    current_step2 = 0
    
    # æ— åŠ¨ä½œï¼ˆåŠ¨ä½œç©ºé—´ä¸­å¿ƒå€¼ï¼Œé€šå¸¸ä¸º0ï¼‰
    no_action = np.array([0])
    
    while current_step2 < total_steps:
        obs2, reward, terminated, truncated, info2 = env2.step(no_action)
        no_action_rewards.append(reward)
        current_step2 += 1
        
        # å®æ—¶è¿›åº¦æ¡æ˜¾ç¤º
        if current_step2 % 100 == 0 or current_step2 == total_steps:
            progress = current_step2 / total_steps
            bar_length = 50
            filled_length = int(bar_length * progress)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            recent_avg = np.mean(no_action_rewards[-1000:]) if len(no_action_rewards) >= 1000 else np.mean(no_action_rewards)
            print(f"\ræ— åŠ¨ä½œæµ‹è¯• [{bar}] {current_step2}/{total_steps} ({progress:.1%}) | å¹³å‡å¥–åŠ±: {recent_avg:.4f}", end='', flush=True)
        
        if terminated or truncated:
            obs2, info2 = env2.reset()
    
    print()  # æ¢è¡Œï¼Œç¡®ä¿è¿›åº¦æ¡æ˜¾ç¤ºå®Œæ•´
    env2.close()
    
    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    model_mean = np.mean(model_rewards)
    model_std = np.std(model_rewards)
    no_action_mean = np.mean(no_action_rewards)
    no_action_std = np.std(no_action_rewards)
    
    improvement = ((model_mean - no_action_mean) / abs(no_action_mean)) * 100 if no_action_mean != 0 else 0
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š å¯¹æ¯”æµ‹è¯•ç»“æœ")
    print("=" * 80)
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹:")
    print(f"   ğŸ† å¹³å‡å¥–åŠ±: {model_mean:.6f} Â± {model_std:.6f}")
    print(f"   ğŸ“ˆ æœ€å¤§å¥–åŠ±: {np.max(model_rewards):.6f}")
    print(f"   ğŸ“‰ æœ€å°å¥–åŠ±: {np.min(model_rewards):.6f}")
    print()
    print(f"â¸ï¸ æ— åŠ¨ä½œ:")
    print(f"   ğŸ† å¹³å‡å¥–åŠ±: {no_action_mean:.6f} Â± {no_action_std:.6f}")
    print(f"   ğŸ“ˆ æœ€å¤§å¥–åŠ±: {np.max(no_action_rewards):.6f}")
    print(f"   ğŸ“‰ æœ€å°å¥–åŠ±: {np.min(no_action_rewards):.6f}")
    print()
    print(f"ğŸ“ˆ æ€§èƒ½æå‡: {improvement:+.2f}%")
    if improvement > 0:
        print("âœ… æ¨¡å‹è¡¨ç°ä¼˜äºæ— åŠ¨ä½œ")
    elif improvement < 0:
        print("âš ï¸ æ¨¡å‹è¡¨ç°ä¸å¦‚æ— åŠ¨ä½œ")
    else:
        print("â– æ¨¡å‹è¡¨ç°ä¸æ— åŠ¨ä½œç›¸å½“")
    print("=" * 80)
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('æ¨¡å‹ vs æ— åŠ¨ä½œ - æ€§èƒ½å¯¹æ¯”', fontsize=16)
    
    # å¥–åŠ±æ›²çº¿å¯¹æ¯”
    axes[0, 0].plot(model_rewards, alpha=0.7, linewidth=0.5, label='ä½¿ç”¨æ¨¡å‹', color='blue')
    axes[0, 0].plot(no_action_rewards, alpha=0.7, linewidth=0.5, label='æ— åŠ¨ä½œ', color='orange')
    axes[0, 0].set_title('æ­¥å¥–åŠ±æ›²çº¿å¯¹æ¯”')
    axes[0, 0].set_xlabel('æ­¥æ•°')
    axes[0, 0].set_ylabel('å¥–åŠ±')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # æ»‘åŠ¨å¹³å‡å¯¹æ¯”
    window_size = 100
    if len(model_rewards) >= window_size:
        model_moving_avg = np.convolve(model_rewards, np.ones(window_size)/window_size, mode='valid')
        no_action_moving_avg = np.convolve(no_action_rewards, np.ones(window_size)/window_size, mode='valid')
        
        axes[0, 1].plot(range(window_size-1, len(model_rewards)), model_moving_avg, 
                       'b-', linewidth=2, label='ä½¿ç”¨æ¨¡å‹')
        axes[0, 1].plot(range(window_size-1, len(no_action_rewards)), no_action_moving_avg, 
                       'orange', linewidth=2, label='æ— åŠ¨ä½œ')
        axes[0, 1].set_title(f'{window_size}æ­¥æ»‘åŠ¨å¹³å‡å¯¹æ¯”')
    else:
        axes[0, 1].plot(model_rewards, 'b-', linewidth=2, label='ä½¿ç”¨æ¨¡å‹')
        axes[0, 1].plot(no_action_rewards, 'orange', linewidth=2, label='æ— åŠ¨ä½œ')
        axes[0, 1].set_title('å¥–åŠ±æ›²çº¿å¯¹æ¯”')
    
    axes[0, 1].set_xlabel('æ­¥æ•°')
    axes[0, 1].set_ylabel('å¹³å‡å¥–åŠ±')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
    axes[1, 0].hist(model_rewards, bins=50, alpha=0.7, label='ä½¿ç”¨æ¨¡å‹', color='blue', density=True)
    axes[1, 0].hist(no_action_rewards, bins=50, alpha=0.7, label='æ— åŠ¨ä½œ', color='orange', density=True)
    axes[1, 0].set_title('å¥–åŠ±åˆ†å¸ƒå¯¹æ¯”')
    axes[1, 0].set_xlabel('å¥–åŠ±å€¼')
    axes[1, 0].set_ylabel('å¯†åº¦')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # ç»Ÿè®¡å¯¹æ¯”æŸ±çŠ¶å›¾
    categories = ['å¹³å‡å¥–åŠ±', 'æ ‡å‡†å·®', 'æœ€å¤§å¥–åŠ±', 'æœ€å°å¥–åŠ±']
    model_stats = [model_mean, model_std, np.max(model_rewards), np.min(model_rewards)]
    no_action_stats = [no_action_mean, no_action_std, np.max(no_action_rewards), np.min(no_action_rewards)]
    
    x = np.arange(len(categories))
    width = 0.35
    
    axes[1, 1].bar(x - width/2, model_stats, width, label='ä½¿ç”¨æ¨¡å‹', color='blue', alpha=0.7)
    axes[1, 1].bar(x + width/2, no_action_stats, width, label='æ— åŠ¨ä½œ', color='orange', alpha=0.7)
    axes[1, 1].set_title('ç»Ÿè®¡æŒ‡æ ‡å¯¹æ¯”')
    axes[1, 1].set_xlabel('ç»Ÿè®¡æŒ‡æ ‡')
    axes[1, 1].set_ylabel('æ•°å€¼')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(categories, rotation=45)
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('./models/comparison_results.png', dpi=300, bbox_inches='tight')
    print("ğŸ“ˆ å¯¹æ¯”ç»“æœå›¾å·²ä¿å­˜è‡³ ./models/comparison_results.png")
    plt.show()
    
    return {
        'model_results': {
            'mean_reward': model_mean,
            'std_reward': model_std,
            'step_rewards': model_rewards
        },
        'no_action_results': {
            'mean_reward': no_action_mean,
            'std_reward': no_action_std,
            'step_rewards': no_action_rewards
        },
        'improvement_percentage': improvement
    }

if __name__ == "__main__":
    try:
        print("LBMç¯å¢ƒæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("æ¯”è¾ƒè®­ç»ƒæ¨¡å‹ä¸æ— åŠ¨ä½œçš„æ€§èƒ½å·®å¼‚")
        
        steps = 5000
        print(f"ä½¿ç”¨æ­¥æ•°: {steps}")
        
        results = run_comparison_test(total_steps=steps)
        
        if results:
            print(f"\nå¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
            print(f"æ¨¡å‹æ€§èƒ½æå‡: {results['improvement_percentage']:+.2f}%")
        
    except Exception as e:
        print(f"å¯¹æ¯”æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()