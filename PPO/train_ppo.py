import gymnasium as gym
import numpy as np
import time
import matplotlib.pyplot as plt
import torch
import gc
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.env_util import make_vec_env
from gymnasium.wrappers import TimeLimit
from env_lbm import LBMEnv


class ProgressCallback(BaseCallback):
    """
    è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼Œç”¨äºå®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦å’Œæ€§èƒ½æŒ‡æ ‡
    """
    def __init__(self, check_freq: int = 1000, verbose=1):
        super(ProgressCallback, self).__init__(verbose)
        self.check_freq = check_freq
        self.start_time = None
        self.episode_rewards = []
        self.episode_lengths = []
        self.drag_coefficients = []
        self.lift_coefficients = []
        
    def _on_training_start(self) -> None:
        self.start_time = time.time()
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print("=" * 60)
        
    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:
            # è®¡ç®—è®­ç»ƒè¿›åº¦
            progress = (self.n_calls / self.locals.get('total_timesteps', 100000)) * 100
            elapsed_time = time.time() - self.start_time
            
            # è·å–æœ€è¿‘çš„è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
            if len(self.model.ep_info_buffer) > 0:
                # è·å–æœ€è¿‘å‡ ä¸ªepisodeçš„å¹³å‡reward
                recent_episodes = list(self.model.ep_info_buffer)[-10:]  # æœ€è¿‘10ä¸ªepisode
                recent_rewards = [ep.get('r', 0) for ep in recent_episodes]
                recent_lengths = [ep.get('l', 0) for ep in recent_episodes]
                
                avg_reward = np.mean(recent_rewards) if recent_rewards else 0
                avg_length = np.mean(recent_lengths) if recent_lengths else 0
                latest_reward = recent_rewards[-1] if recent_rewards else 0
                
                self.episode_rewards.append(latest_reward)
                self.episode_lengths.append(recent_lengths[-1] if recent_lengths else 0)
                
                # å°è¯•è·å–ç¯å¢ƒä¿¡æ¯
                cd, cl = 0, 0
                try:
                    # ä»æœ€è¿‘çš„episodeä¿¡æ¯ä¸­è·å–CDå’ŒCL
                    if len(recent_episodes) > 0 and 'CD' in recent_episodes[-1]:
                        cd = recent_episodes[-1].get('CD', 0)
                        cl = recent_episodes[-1].get('CL', 0)
                        self.drag_coefficients.append(cd)
                        self.lift_coefficients.append(cl)
                except:
                    pass
                
                # æ‰“å°è¯¦ç»†çš„è®­ç»ƒä¿¡æ¯
                print(f"ğŸ¯ ã€ç¬¬ {self.n_calls:,} æ­¥ã€‘")
                print(f"   ğŸ“ˆ è¿›åº¦: {progress:.1f}% | â±ï¸  ç”¨æ—¶: {elapsed_time:.0f}s")
                print(f"   ğŸ† æœ€æ–°å¥–åŠ±: {latest_reward:.6f}")
                print(f"   ğŸ“Š å¹³å‡å¥–åŠ±(æœ€è¿‘10è½®): {avg_reward:.6f}")
                print(f"   ğŸ“ å¹³å‡é•¿åº¦: {avg_length:.1f} æ­¥")
                if cd != 0 or cl != 0:
                    print(f"   ğŸŒªï¸  é˜»åŠ›ç³»æ•°(CD): {cd:.6f} | å‡åŠ›ç³»æ•°(CL): {cl:.6f}")
                    print(f"   âš¡ å‡é˜»æ¯”(CL/CD): {cl/cd:.4f}" if cd > 0 else "   âš¡ å‡é˜»æ¯”: N/A")
                print("-" * 50)
            else:
                print(f"ğŸ“Š æ­¥æ•°: {self.n_calls:,} | è¿›åº¦: {progress:.1f}% | "
                      f"æ—¶é—´: {elapsed_time:.0f}s | ç­‰å¾…episodeå®Œæˆ...")
                
        return True
    
    def _on_training_end(self) -> None:
        total_time = time.time() - self.start_time
        print("=" * 60)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.0f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        if len(self.episode_rewards) > 0:
            self.plot_training_curves()
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('PPO è®­ç»ƒè¿‡ç¨‹ç›‘æ§', fontsize=16)
        
        # å¥–åŠ±æ›²çº¿
        if len(self.episode_rewards) > 0:
            axes[0, 0].plot(self.episode_rewards)
            axes[0, 0].set_title('Episode Rewards')
            axes[0, 0].set_xlabel('Episodes')
            axes[0, 0].set_ylabel('Reward')
            axes[0, 0].grid(True)
        
        # Episode é•¿åº¦
        if len(self.episode_lengths) > 0:
            axes[0, 1].plot(self.episode_lengths)
            axes[0, 1].set_title('Episode Lengths')
            axes[0, 1].set_xlabel('Episodes')
            axes[0, 1].set_ylabel('Steps')
            axes[0, 1].grid(True)
        
        # é˜»åŠ›ç³»æ•°
        if len(self.drag_coefficients) > 0:
            axes[1, 0].plot(self.drag_coefficients)
            axes[1, 0].set_title('Drag Coefficient (CD)')
            axes[1, 0].set_xlabel('Episodes')
            axes[1, 0].set_ylabel('CD')
            axes[1, 0].grid(True)
        
        # å‡åŠ›ç³»æ•°
        if len(self.lift_coefficients) > 0:
            axes[1, 1].plot(self.lift_coefficients)
            axes[1, 1].set_title('Lift Coefficient (CL)')
            axes[1, 1].set_xlabel('Episodes')
            axes[1, 1].set_ylabel('CL')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig('./models/training_curves.png', dpi=300, bbox_inches='tight')
        print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ ./models/training_curves.png")
        plt.show()

def main():
        
    if torch.cuda.is_available():
        torch.set_default_tensor_type('torch.cuda.FloatTensor')
        device = torch.device('cuda')
        print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        torch.cuda.empty_cache()
        gc.collect()
    else:
        device = torch.device('cpu')
    

    # åˆ›å»ºç¯å¢ƒå®ä¾‹ï¼Œè®¾ç½®episodeæœ€å¤§é•¿åº¦
    base_env = LBMEnv(config={"max_episode_steps": 2000})

    env = TimeLimit(base_env, max_episode_steps=2000)  


    # é…ç½®æ—¥å¿—è®°å½•å™¨
    new_logger = configure("./models/logs/", ["stdout", "csv", "tensorboard"])
    
    # åˆ›å»º PPO æ¨¡å‹ 
    model = PPO(
        policy="MlpPolicy",          # ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºç­–ç•¥ç½‘ç»œ
        env=env,                     # ä½¿ç”¨è‡ªå®šä¹‰çš„LBMç¯å¢ƒ
        verbose=1,                   # è¾“å‡ºè®­ç»ƒä¿¡æ¯
        tensorboard_log="./ppo_lbm_tensorboard/",  # Tensorboard æ—¥å¿—ç›®å½•
        learning_rate=3e-4,          # å­¦ä¹ ç‡
        n_steps=1024,                # æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
        batch_size=32,               # æ‰¹æ¬¡å¤§å°
        n_epochs=5,                  # æ¯æ¬¡æ›´æ–°çš„è½®æ•°
        gamma=0.999,                  # æŠ˜æ‰£å› å­
        gae_lambda=0.95,             # GAE lambda
        clip_range=0.2,              # PPO clip range
        ent_coef=0.01,               # ç†µç³»æ•°
    )
    
    # è®¾ç½®è‡ªå®šä¹‰æ—¥å¿—è®°å½•å™¨
    model.set_logger(new_logger)

    # åˆ›å»ºå›è°ƒå‡½æ•°
    progress_callback = ProgressCallback(check_freq=1000, verbose=1)
    
    eval_callback = EvalCallback(
        env,
        best_model_save_path="./models/best_model/",
        log_path="./models/eval_log/",
        eval_freq=2000,
        deterministic=True,
        render=False,
        verbose=1
    )
    
    # ç»„åˆå›è°ƒå‡½æ•°
    callbacks = [progress_callback, eval_callback]
    
    print("å¼€å§‹è®­ç»ƒ...")

    model.learn(
        total_timesteps=20000,      # æ€»è®­ç»ƒæ­¥æ•°
        callback=callbacks,          # å›è°ƒå‡½æ•°åˆ—è¡¨
        progress_bar=True           # æ˜¾ç¤ºè¿›åº¦æ¡
    )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save("./models/final_model")
    print("\næ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜è‡³ ./models/final_model")

if __name__ == '__main__':
    main()
